{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activity allows you to practice using the [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) library to scrape some data from the web. It also allows you to practice using a **Jupyter Notebook** to both document and perform your work. As you can see, you can write _Markdown_, as well as Python\n",
    "\n",
    "_(quick tip: hit `esc` then `m` to start writing Markdown rather than Python. Then, hit `shift` and `enter` to run a code section)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Beautiful soup library, we'll need to ensure it's installed on your machine. You can do this easily by running the following command on your terminal \n",
    "\n",
    "```\n",
    "# Install beautifulsoup using pip on the terminal\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to import the library inside of this notebook by running the following line of Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs, SoupStrainer as ss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to import a few other libraries, such as `pandas` to manage our data, and `requests` to make URL requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import pandas as p\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Institution Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to use python to identify the **links to institution pages** on their [website](https://collegecost.ed.gov/catc/Default.aspx). We'll begin by making a request of the page content. Due to peculiarities of how the page is built on the client side, we'll read a local version of the page using the `codecs` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "file = codecs.open(\"college-site.html\", 'r')\n",
    "page_content = file.read()\n",
    "soup = bs(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have all the page content, you should open up the [website](https://collegecost.ed.gov/catc/Default.aspx) in your browser to _identify the part of the DOM_ where the relevant information is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the TuitionGrid table\n",
    "table = soup.find(id = 'dvCATWTuitionGrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract each row from the table\n",
    "table_rows = table.find_all('tr', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://nces.ed.gov/collegenavigator/?id=142328']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a single row\n",
    "table_rows[0]['onclick']\n",
    "re.findall(r\"'(.*?)'\", table_rows[0]['onclick'], re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting links from table rows\n",
    "\n",
    "In this section, we'll iterate through the table rows and extract the links from each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to extract url\n",
    "def extract_url(row):\n",
    "    links = re.findall(r\"'(.*?)'\", row['onclick'], re.DOTALL)\n",
    "    return links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# List to store links\n",
    "links = []\n",
    "for tr in table_rows:\n",
    "    i += 1\n",
    "    link = extract_url(tr)\n",
    "    links.append(link)\n",
    "print len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through links and extract content from webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
